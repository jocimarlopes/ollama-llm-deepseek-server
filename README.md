# ğŸ§  Flask API com DeepSeek-R1 via Ollama

Esta Ã© uma API desenvolvida com Flask em Python, conectando-se ao modelo LLM **DeepSeek-R1** utilizando a plataforma [Ollama](https://ollama.com/).  
Ela permite realizar inferÃªncias com LLMs de forma simples, eficiente e local com uma UI bÃ¡sica e retorno stream de resposta.

## âš™ï¸ PrÃ©-requisitos

Antes de rodar o projeto, certifique-se de ter os seguintes itens instalados:

- [Python 3.8+](https://www.python.org/downloads/)
- [Ollama](https://ollama.com/)
- `deepseek-r1` baixado no Ollama

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**

```bash
git clone https://github.com/jocimarlopes/ollama-llm-deepseek-server.git
cd ollama-llm-deepseek-server
```

2. **Instale as dependÃªncias**

```bash
pip install -r requirements.txt
```

3. **Instale e inicie o Ollama**
Se ainda nÃ£o tiver o Ollama:
```bash
# No Linux via curl
curl -fsSL https://ollama.com/install.sh | sh
```

4. **Baixe o modelo DeepSeek-R1**
Se ainda nÃ£o tiver o Ollama:
```bash
ollama pull deepseek-r1:latest
```

## ğŸ§ª Executando a API

Basta rodar o script app.py:

```bash
python app.py
```

## ğŸ“ Estrutura bÃ¡sica

Basta rodar o script app.py:

```bash
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ‘¤ CrÃ©ditos
Projeto desenvolvido por Jocimar Lopes.
Sinta-se Ã  vontade para contribuir ou utilizar em seus prÃ³prios projetos.

## ğŸ“„ LicenÃ§a
Este projeto estÃ¡ licenciado sob os termos da MIT License.
